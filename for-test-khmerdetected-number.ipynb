{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f46ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T13:06:22.910192Z",
     "iopub.status.busy": "2025-05-07T13:06:22.909755Z",
     "iopub.status.idle": "2025-05-07T15:03:09.494033Z",
     "shell.execute_reply": "2025-05-07T15:03:09.492780Z"
    },
    "papermill": {
     "duration": 7006.591595,
     "end_time": "2025-05-07T15:03:09.496015",
     "exception": false,
     "start_time": "2025-05-07T13:06:22.904420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data from /kaggle/input/fortest/normalization_augmentation.sav...\n",
      "Successfully loaded data. Number of samples: 20579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences and labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20579/20579 [00:02<00:00, 7112.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20579 sequences with 20579 labels.\n",
      "Train: 16463, Val: 2058, Test: 2058\n",
      "Starting training with params: {'lr': 0.001, 'epochs': 10, 'gmm_weight': 0.05, 'growth_rate': 32, 'dense_layers': 3, 'hidden_size': 128, 'num_gru_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.9217, Val Loss: 1.7412\n",
      "‚úÖ New best validation loss: 1.7412.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 1.5803, Val Loss: 1.4246\n",
      "‚úÖ New best validation loss: 1.4246.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 1.4166, Val Loss: 1.2912\n",
      "‚úÖ New best validation loss: 1.2912.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 1.2663, Val Loss: 1.2062\n",
      "‚úÖ New best validation loss: 1.2062.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 1.1413, Val Loss: 0.9192\n",
      "‚úÖ New best validation loss: 0.9192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 1.0432, Val Loss: 1.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.9415, Val Loss: 0.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.7994, Val Loss: 5.0254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.7104, Val Loss: 0.4835\n",
      "‚úÖ New best validation loss: 0.4835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.5856, Val Loss: 0.5323\n",
      "Loaded best model state based on validation loss.\n",
      "Training finished. Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7143\n",
      "Test Precision: 0.7212\n",
      "Test Recall: 0.7027\n",
      "Test F1-score: 0.7022\n",
      "‚úÖ Saved final best model to khmer_model_results_optimized/best_model_val_loss_20250507_150309.pt\n",
      "‚úÖ Saved model metadata to best_model_val_loss_20250507_150309.txt\n",
      "Saved loss plot to khmer_model_results_optimized/loss_plot_20250507_150309.png\n",
      "\n",
      "üèÜ Final Optimized Model Results:\n",
      "Parameters: {'lr': 0.001, 'epochs': 10, 'gmm_weight': 0.05, 'growth_rate': 32, 'dense_layers': 3, 'hidden_size': 128, 'num_gru_layers': 2}\n",
      "Best Validation Loss achieved: 0.4835\n",
      "Test Metrics:\n",
      "  accuracy: 0.7143\n",
      "  precision: 0.7212\n",
      "  recall: 0.7027\n",
      "  f1: 0.7022\n",
      "\n",
      "All results saved in ./khmer_model_results_optimized\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Use Agg backend for non-interactive plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Ensure compatibility with loading .sav files\n",
    "if hasattr(np, 'core') and hasattr(np.core, 'multiarray') and hasattr(np.core.multiarray, 'scalar'):\n",
    "    torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
    "elif hasattr(np, '_core') and hasattr(np._core, 'multiarray') and hasattr(np._core.multiarray, 'scalar'):\n",
    "    torch.serialization.add_safe_globals([np._core.multiarray.scalar])\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Class Definitions ---\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, gru_out):\n",
    "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
    "        weighted = gru_out * attn_weights\n",
    "        return weighted.sum(dim=1)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, input_size, growth_rate, num_layers, dropout_rate=0.3):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(input_size + i * growth_rate),\n",
    "                    nn.Linear(input_size + i * growth_rate, growth_rate * 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(growth_rate * 4, growth_rate),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                )\n",
    "            )\n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            new_features = layer(torch.cat(features, dim=-1))\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, dim=-1)\n",
    "\n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.3):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(input_size),\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EnhancedKhmerDigitGRU(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=256, num_layers=4, \n",
    "                 output_size=10, dropout_rate=0.3, gmm_components=10,\n",
    "                 growth_rate=32, dense_layers=4):\n",
    "        super(EnhancedKhmerDigitGRU, self).__init__()\n",
    "        self.gmm_components = gmm_components\n",
    "        self.input_feature_size = input_size\n",
    "        \n",
    "        self.initial_dense = DenseBlock(self.input_feature_size + 2, growth_rate, dense_layers, dropout_rate)\n",
    "        current_size = self.input_feature_size + 2 + growth_rate * dense_layers\n",
    "        \n",
    "        self.transition1 = TransitionLayer(current_size, hidden_size // 2, dropout_rate)\n",
    "        current_size = hidden_size // 2\n",
    "        \n",
    "        self.gru = nn.GRU(current_size, hidden_size, num_layers, \n",
    "                         batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
    "        \n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        \n",
    "        self.final_dense = DenseBlock(hidden_size * 2, growth_rate, dense_layers, dropout_rate)\n",
    "        final_size = hidden_size * 2 + growth_rate * dense_layers\n",
    "        \n",
    "        self.fc_class = nn.Sequential(\n",
    "            nn.LayerNorm(final_size),\n",
    "            nn.Linear(final_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        self.fc_gmm = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size * 2), \n",
    "            nn.Linear(hidden_size * 2, gmm_components * 5)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def preprocess_input(self, x): \n",
    "        batch_size, seq_len, current_input_features = x.shape\n",
    "        delta = torch.zeros(batch_size, seq_len, 2, device=x.device) \n",
    "        if seq_len > 1:\n",
    "            delta[:, 1:, 0] = x[:, 1:, 0] - x[:, :-1, 0]\n",
    "            delta[:, 1:, 1] = x[:, 1:, 1] - x[:, :-1, 1]\n",
    "        return torch.cat([x, delta], dim=-1)\n",
    "\n",
    "    def gmm_loss(self, gmm_params, targets):\n",
    "        batch_size, seq_len_gmm_out, _ = gmm_params.shape\n",
    "        _, seq_len_targets, _ = targets.shape \n",
    "\n",
    "        if seq_len_gmm_out == 0 or seq_len_targets == 0:\n",
    "             return torch.tensor(0.0, device=gmm_params.device, requires_grad=True)\n",
    "\n",
    "        M = self.gmm_components\n",
    "        gmm_params_reshaped = gmm_params.view(batch_size, seq_len_gmm_out, M, 5)\n",
    "\n",
    "        pi_logits = gmm_params_reshaped[..., 0]\n",
    "        mu_x = gmm_params_reshaped[..., 1]\n",
    "        mu_y = gmm_params_reshaped[..., 2]\n",
    "        log_sigma_x = gmm_params_reshaped[..., 3]\n",
    "        log_sigma_y = gmm_params_reshaped[..., 4]\n",
    "\n",
    "        pi = self.softmax(pi_logits)\n",
    "        sigma_x = torch.exp(log_sigma_x)\n",
    "        sigma_y = torch.exp(log_sigma_y)\n",
    "\n",
    "        targets_expanded = targets.unsqueeze(2)\n",
    "\n",
    "        x_term_exp = ((targets_expanded[..., 0] - mu_x) / (sigma_x + 1e-8))**2\n",
    "        y_term_exp = ((targets_expanded[..., 1] - mu_y) / (sigma_y + 1e-8))**2\n",
    "        \n",
    "        log_pdf_x = -0.5 * x_term_exp - log_sigma_x - 0.5 * torch.log(torch.tensor(2 * torch.pi, device=gmm_params.device))\n",
    "        log_pdf_y = -0.5 * y_term_exp - log_sigma_y - 0.5 * torch.log(torch.tensor(2 * torch.pi, device=gmm_params.device))\n",
    "        \n",
    "        log_likelihood_components = log_pdf_x + log_pdf_y\n",
    "        \n",
    "        log_pi = torch.log(pi + 1e-10)\n",
    "        sum_terms = log_pi + log_likelihood_components\n",
    "        \n",
    "        log_prob_sum = torch.logsumexp(sum_terms, dim=2)\n",
    "        \n",
    "        return -log_prob_sum.mean()\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        x_aug = self.preprocess_input(x)\n",
    "        x_dense = self.initial_dense(x_aug)\n",
    "        x_trans = self.transition1(x_dense)\n",
    "        \n",
    "        gru_out, _ = self.gru(x_trans)\n",
    "        gru_out_dp = self.dropout(gru_out)\n",
    "        \n",
    "        attention_out = self.attention(gru_out_dp)\n",
    "        \n",
    "        final_features_input = attention_out.unsqueeze(1)\n",
    "        final_features = self.final_dense(final_features_input).squeeze(1)\n",
    "        \n",
    "        class_out = self.fc_class(final_features)\n",
    "        gmm_out = self.fc_gmm(gru_out_dp)\n",
    "            \n",
    "        return class_out, gmm_out\n",
    "\n",
    "# --- Dataset and Data Loading ---\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_sequences, labels):\n",
    "        self.data_sequences = data_sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_sequences[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_sequences, labels, lengths\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_model(model, train_loader, val_loader, criterion_class, optimizer, device, epochs, gmm_weight=0.1, results_dir=\"results\"):\n",
    "    model.to(device)\n",
    "    train_losses_epoch, val_losses_epoch = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [T]\", leave=False)\n",
    "        for inputs, labels, lengths in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            class_out, gmm_out = model(inputs, lengths=lengths)\n",
    "            loss_class = criterion_class(class_out, labels)\n",
    "            loss_gmm = torch.tensor(0.0, device=device)\n",
    "            if inputs.shape[1] > 1:\n",
    "                actual_deltas = torch.zeros(inputs.shape[0], inputs.shape[1] - 1, 2, device=device)\n",
    "                actual_deltas[..., 0] = inputs[:, 1:, 0] - inputs[:, :-1, 0]\n",
    "                actual_deltas[..., 1] = inputs[:, 1:, 1] - inputs[:, :-1, 1]\n",
    "                gmm_out_for_loss = gmm_out[:, :-1, :] \n",
    "                if gmm_out_for_loss.shape[1] == actual_deltas.shape[1] and gmm_out_for_loss.shape[1] > 0:\n",
    "                    loss_gmm = model.gmm_loss(gmm_out_for_loss, actual_deltas)\n",
    "            \n",
    "            loss = loss_class + gmm_weight * loss_gmm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), cls=loss_class.item(), gmm=loss_gmm.item())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses_epoch.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [V]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, lengths in val_progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                class_out, gmm_out_val = model(inputs, lengths=lengths)\n",
    "                loss_class_val = criterion_class(class_out, labels)\n",
    "                loss_gmm_val = torch.tensor(0.0, device=device)\n",
    "                if inputs.shape[1] > 1:\n",
    "                    actual_deltas_val = torch.zeros(inputs.shape[0], inputs.shape[1] - 1, 2, device=device)\n",
    "                    actual_deltas_val[..., 0] = inputs[:, 1:, 0] - inputs[:, :-1, 0]\n",
    "                    actual_deltas_val[..., 1] = inputs[:, 1:, 1] - inputs[:, :-1, 1]\n",
    "                    gmm_out_val_for_loss = gmm_out_val[:, :-1, :]\n",
    "                    if gmm_out_val_for_loss.shape[1] == actual_deltas_val.shape[1] and gmm_out_val_for_loss.shape[1] > 0:\n",
    "                        loss_gmm_val = model.gmm_loss(gmm_out_val_for_loss, actual_deltas_val)\n",
    "                current_val_sample_loss = loss_class_val + gmm_weight * loss_gmm_val\n",
    "                val_running_loss += current_val_sample_loss.item()\n",
    "                val_progress_bar.set_postfix(val_loss=current_val_sample_loss.item())\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_losses_epoch.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"‚úÖ New best validation loss: {avg_val_loss:.4f}.\")\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state based on validation loss.\")\n",
    "    return train_losses_epoch, val_losses_epoch, model\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_model(model, test_loader, device, verbose=True):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_preds, all_labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, lengths in tqdm(test_loader, desc=\"Evaluating Test Set\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            class_out, _ = model(inputs, lengths=lengths)\n",
    "            preds = torch.argmax(class_out, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels_list.extend(labels.cpu().numpy())\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels_list, all_preds),\n",
    "        'precision': precision_score(all_labels_list, all_preds, average=\"macro\", zero_division=0),\n",
    "        'recall': recall_score(all_labels_list, all_preds, average=\"macro\", zero_division=0),\n",
    "        'f1': f1_score(all_labels_list, all_preds, average=\"macro\", zero_division=0)\n",
    "    }\n",
    "    if verbose:\n",
    "        print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Test Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"Test F1-score: {metrics['f1']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "# --- Save Model Metadata ---\n",
    "def save_model_metadata(model_path, params, metrics, results_dir):\n",
    "    metadata_filename = os.path.splitext(os.path.basename(model_path))[0] + \".txt\"\n",
    "    with open(os.path.join(results_dir, metadata_filename), \"w\") as f:\n",
    "        f.write(f\"Parameters:\\n{params}\\n\\n\")\n",
    "        f.write(f\"Metrics:\\n\")\n",
    "        for k, v in metrics.items():\n",
    "            f.write(f\"  {k}: {v}\\n\")\n",
    "    print(f\"‚úÖ Saved model metadata to {metadata_filename}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Correct path to the .sav file\n",
    "    data_path = \"/kaggle/input/fortest/normalization_augmentation.sav\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    \n",
    "    try:\n",
    "        all_sequences_raw = torch.load(data_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "        print(f\"Successfully loaded data. Number of samples: {len(all_sequences_raw)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Class distribution from the dataset\n",
    "    class_counts = [2444, 1898, 1924, 2249, 1911, 1846, 2145, 2002, 1859, 2301]\n",
    "    all_labels_list_generated = []\n",
    "    for i, count in enumerate(class_counts):\n",
    "        all_labels_list_generated.extend([i] * count)\n",
    "    \n",
    "    if len(all_labels_list_generated) != len(all_sequences_raw):\n",
    "        raise ValueError(f\"Mismatch in number of labels ({len(all_labels_list_generated)}) and sequences ({len(all_sequences_raw)})\")\n",
    "\n",
    "    # Process sequences and labels\n",
    "    final_sequences = []\n",
    "    final_labels = []\n",
    "    \n",
    "    for i, sample_entry in enumerate(tqdm(all_sequences_raw, desc=\"Processing sequences and labels\")):\n",
    "        if not (isinstance(sample_entry, list) and len(sample_entry) >= 2):\n",
    "            continue\n",
    "\n",
    "        list_of_normalized_strokes = sample_entry[1]\n",
    "\n",
    "        if not isinstance(list_of_normalized_strokes, list):\n",
    "            continue\n",
    "\n",
    "        flat_sequence_points_for_digit = []\n",
    "        for stroke_as_list_of_points in list_of_normalized_strokes:\n",
    "            if not isinstance(stroke_as_list_of_points, list):\n",
    "                continue\n",
    "            \n",
    "            for point_tuple in stroke_as_list_of_points:\n",
    "                if isinstance(point_tuple, (tuple, list)) and len(point_tuple) == 2:\n",
    "                    try:\n",
    "                        x_coord = float(point_tuple[0])\n",
    "                        y_coord = float(point_tuple[1])\n",
    "                        flat_sequence_points_for_digit.append((x_coord, y_coord))\n",
    "                    except (ValueError, TypeError, IndexError):\n",
    "                        continue\n",
    "        \n",
    "        if flat_sequence_points_for_digit:\n",
    "            final_sequences.append(torch.tensor(flat_sequence_points_for_digit, dtype=torch.float32))\n",
    "            final_labels.append(all_labels_list_generated[i])\n",
    "\n",
    "    print(f\"Processed {len(final_sequences)} sequences with {len(final_labels)} labels.\")\n",
    "    if not final_sequences:\n",
    "        raise ValueError(\"No sequences were successfully processed.\")\n",
    "\n",
    "    # Split data\n",
    "    train_sequences, temp_sequences, train_labels, temp_labels = train_test_split(\n",
    "        final_sequences, final_labels, test_size=0.2, stratify=final_labels, random_state=42\n",
    "    )\n",
    "    val_sequences, test_sequences, val_labels, test_labels = train_test_split(\n",
    "        temp_sequences, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42\n",
    "    )\n",
    "    print(f\"Train: {len(train_sequences)}, Val: {len(val_sequences)}, Test: {len(test_sequences)}\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = CustomDataset(train_sequences, train_labels)\n",
    "    val_dataset = CustomDataset(val_sequences, val_labels)\n",
    "    test_dataset = CustomDataset(test_sequences, test_labels)\n",
    "\n",
    "    batch_size = 16\n",
    "    num_workers_dl = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                            collate_fn=collate_fn, pin_memory=True if device.type == 'cuda' else False, \n",
    "                            num_workers=num_workers_dl)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                          collate_fn=collate_fn, pin_memory=True if device.type == 'cuda' else False, \n",
    "                          num_workers=num_workers_dl)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                           collate_fn=collate_fn, pin_memory=True if device.type == 'cuda' else False, \n",
    "                           num_workers=num_workers_dl)\n",
    "    \n",
    "    # Model parameters\n",
    "    current_params = {\n",
    "        \"lr\": 0.001,\n",
    "        \"epochs\": 10,  # Increased from 1 to get better results\n",
    "        \"gmm_weight\": 0.05,\n",
    "        \"growth_rate\": 32,\n",
    "        \"dense_layers\": 3,\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_gru_layers\": 2 \n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedKhmerDigitGRU(\n",
    "        input_size=2,\n",
    "        hidden_size=current_params['hidden_size'],\n",
    "        num_layers=current_params['num_gru_layers'],\n",
    "        output_size=10,\n",
    "        dropout_rate=0.3,\n",
    "        gmm_components=10,\n",
    "        growth_rate=current_params['growth_rate'],\n",
    "        dense_layers=current_params['dense_layers']\n",
    "    )\n",
    "    \n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=current_params['lr'])\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = \"khmer_model_results_optimized\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Starting training with params: {current_params}\")\n",
    "    train_losses, val_losses, best_trained_model = train_model(\n",
    "        model, train_loader, val_loader, criterion_class, optimizer, device,\n",
    "        current_params['epochs'], current_params['gmm_weight'], results_dir=results_dir\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Training finished. Evaluating on test set...\")\n",
    "    final_metrics = evaluate_model(best_trained_model, test_loader, device, verbose=True)\n",
    "    \n",
    "    # Save model and metadata\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_model_filename = f\"best_model_val_loss_{timestamp}.pt\"\n",
    "    final_model_path = os.path.join(results_dir, final_model_filename)\n",
    "    torch.save(best_trained_model.state_dict(), final_model_path)\n",
    "    print(f\"‚úÖ Saved final best model to {final_model_path}\")\n",
    "    save_model_metadata(final_model_path, current_params, final_metrics, results_dir)\n",
    "\n",
    "    # Plot training curves\n",
    "    if current_params['epochs'] > 0 and train_losses and val_losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Losses')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        loss_plot_path = os.path.join(results_dir, f\"loss_plot_{timestamp}.png\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        print(f\"Saved loss plot to {loss_plot_path}\")\n",
    "    else:\n",
    "        print(\"Skipping loss plot generation as epochs was 0 or no losses recorded.\")\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nüèÜ Final Optimized Model Results:\")\n",
    "    print(f\"Parameters: {current_params}\")\n",
    "    if val_losses:\n",
    "        print(f\"Best Validation Loss achieved: {min(val_losses):.4f}\")\n",
    "    print(\"Test Metrics:\")\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(f\"\\nAll results saved in ./{results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7346254,
     "sourceId": 11703839,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7015.622645,
   "end_time": "2025-05-07T15:03:13.470634",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-07T13:06:17.847989",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
